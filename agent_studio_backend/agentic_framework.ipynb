{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#### In router agent allow multiple async calls to top-k agents\n",
    "#### Components - Deterministic or Probabilistic\n",
    "#### Add timeout to agent calls\n",
    "#### Tools and components are very similar but components are what we control and tools are external services\n",
    "\n",
    "#### Add ability to call any Rest API as a tool: URL, input, output, types of input and output. So we can\n",
    "#### connect to any external service. For example, we can connect to an API to send out prompt to evaluate it.\n",
    "#### Command agent can call any agent, tool, or component. It can also call a router agent to call multiple agents.\n",
    "#### Command agent should be able to handle all types of inputs and outputs. Text, Image and Voice.\n",
    "#### Have the ability to expose the various parts of the Command agent as a REST API.\n",
    "#### Add logging and monitoring to the agent. Log all the inputs and outputs. Log the time taken for each call.\n",
    "#### Example types to be implemented: CX, Synthetic data generation (contract, customer chat generation, csv data), Media planning, Invoice extraction,\n",
    "#### Create a generic API calling tool.\n",
    "#### Add a validation tool to validate the output of the agent. For example, if the agent is supposed to return a number, then validate that the output is a number. Also verify that the variables are sent back to the agents correctly. And the models are not hallucinating."
   ],
   "id": "c1f165c00da4d3bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function schema decorator\n",
   "id": "56f0617d98706373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import inspect\n",
    "import json\n",
    "from typing import Any, Dict, get_type_hints, List, Union, Optional\n",
    "\n",
    "from nltk.inference.mace import arguments\n",
    "\n",
    "\n",
    "def function_schema(func):\n",
    "    \"\"\"\n",
    "    Decorator that generates an OpenAI function schema and attaches it to the function.\n",
    "    \"\"\"\n",
    "    def python_function_to_openai_schema(func) -> Dict[str, Any]:\n",
    "        \"\"\"Converts a function into an OpenAI JSON schema.\"\"\"\n",
    "        signature = inspect.signature(func)\n",
    "        type_hints = get_type_hints(func)\n",
    "\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\":{\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__ or f\"Function {func.__name__}\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "\n",
    "        for param_name, param in signature.parameters.items():\n",
    "            param_type = type_hints.get(param_name, Any)\n",
    "            openai_type, is_optional = python_type_to_openai_type(param_type)\n",
    "\n",
    "            # schema[\"parameters\"][\"properties\"][param_name] = {\n",
    "            schema[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n",
    "                \"type\": openai_type,\n",
    "                \"description\": f\"{param_name} parameter\"\n",
    "            }\n",
    "\n",
    "            # Add to required list only if it's not Optional and has no default value\n",
    "            if not is_optional and param.default is inspect.Parameter.empty:\n",
    "                # schema[\"parameters\"][\"required\"].append(param_name)\n",
    "                schema[\"function\"][\"parameters\"][\"required\"].append(param_name)\n",
    "\n",
    "        return schema\n",
    "\n",
    "    def python_type_to_openai_type(py_type) -> (str, bool):\n",
    "        \"\"\"Maps Python types to OpenAI JSON schema types, supporting Optional and List.\"\"\"\n",
    "        from typing import get_origin, get_args\n",
    "\n",
    "        # Handle Optional[X] (Union[X, None])\n",
    "        if get_origin(py_type) is Union:\n",
    "            args = get_args(py_type)\n",
    "            non_none_types = [t for t in args if t is not type(None)]\n",
    "            if len(non_none_types) == 1:\n",
    "                openai_type, _ = python_type_to_openai_type(non_none_types[0])\n",
    "                return openai_type, True  # It's Optional\n",
    "\n",
    "        # Handle List[X]\n",
    "        if get_origin(py_type) is list or get_origin(py_type) is List:\n",
    "            return \"array\", False\n",
    "\n",
    "        # Base type mapping\n",
    "        mapping = {\n",
    "            int: \"integer\",\n",
    "            float: \"number\",\n",
    "            str: \"string\",\n",
    "            bool: \"boolean\",\n",
    "            dict: \"object\",\n",
    "        }\n",
    "\n",
    "        return mapping.get(py_type, \"string\"), False  # Default to string\n",
    "\n",
    "    # Attach schema as a function attribute\n",
    "    func.openai_schema = python_function_to_openai_schema(func)\n",
    "    return func\n",
    "\n"
   ],
   "id": "cd6ac879e167931"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Logging decorator",
   "id": "4b669cd475ef6d44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import functools\n",
    "from typing import Any, Callable\n",
    "\n",
    "def log_execution(func: Callable) -> Callable:\n",
    "    \"\"\"Decorator to log the input and output of a function.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs) -> Any:\n",
    "        self.logs[\"input\"].append(kwargs)\n",
    "        try:\n",
    "            result = func(self, *args, **kwargs)\n",
    "            self.logs[\"output\"].append(result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.logs[\"output\"].append(f\"Error: {e}\")\n",
    "            raise\n",
    "    return wrapper"
   ],
   "id": "57ef988137d28ec4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAI Credentials and Client",
   "id": "129af31922a31dc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\".env.local\")\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "CX_ID = os.getenv(\"CX_ID_PERSONAL\")\n",
    "GOOGLE_API = os.getenv(\"GOOGLE_API_PERSONAL\")\n"
   ],
   "id": "25151e7ef0c60971"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tools",
   "id": "612534dacc3b72cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import markdownify\n",
    "\n",
    "\n",
    "@function_schema\n",
    "def add_numbers(a: int, b: int) -> str:\n",
    "    \"\"\"\n",
    "    Adds two numbers and returns the sum.\n",
    "    \"\"\"\n",
    "    return f\"The sum of {a} and {b} is {a + b}\"\n",
    "\n",
    "@function_schema\n",
    "def get_customer_order(customer_id: int) -> str:\n",
    "    \"\"\"\"\n",
    "    Returns the order number for a given customer ID.\"\"\"\n",
    "    if customer_id == 1111:\n",
    "        return f\"The order number for customer ID {customer_id} is 1720\"\n",
    "    elif customer_id == 2222:\n",
    "        return f\"The order number for customer ID {customer_id} is 9\"\n",
    "    return f\"No order found for customer ID {customer_id}\"\n",
    "\n",
    "@function_schema\n",
    "def weather_forecast(location: str) -> str:\n",
    "    \"\"\"\"\n",
    "    Returns the weather forecast for a given location.\n",
    "    \"\"\"\n",
    "    return \"It's sunny\" if \"a\" in location else \"It's rainy\"\n",
    "\n",
    "@function_schema\n",
    "def url_to_markdown(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content = soup.find('body')\n",
    "\n",
    "        if content:\n",
    "            markdown_content = markdownify.markdownify(str(content), heading_style=\"ATX\")\n",
    "            return markdown_content[:20000]\n",
    "        else:\n",
    "            return \"No content found in the webpage body.\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "\n",
    "@function_schema\n",
    "def web_search(query: str,num: Optional[int]=2) -> str:\n",
    "    \"\"\"\n",
    "    You can use this tool to get any information from the web. Just type in your query and get the results.\n",
    "    \"\"\"\n",
    "    num=1\n",
    "    # try:\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API}&cx={CX_ID}&num={num}\"\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    urls = [i[\"link\"] for i in response.json()[\"items\"]]\n",
    "    print(urls)\n",
    "    text = \"\\n\".join([url_to_markdown(i) for i in urls])\n",
    "    # print(text)\n",
    "    prompt = f\"Use the following text to answer the given: {query} \\n\\n ---BEGIN WEB TEXT --- {text} ---BEGIN WEB TEXT --- \"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"You're a web search agent.\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "    # return f\"\"\"\n",
    "    # Here are the web results for your query: {response.choices[0].message.content}\n",
    "    # \"\"\"\n",
    "    # except:\n",
    "    #     return \"Error fetching search results.\"\n"
   ],
   "id": "ca9fada9b6a8e1fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "web_search.openai_schema",
   "id": "636211a461652cce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tool_store = {\n",
    "    \"add_numbers\": add_numbers,\n",
    "    \"weather_forecast\": weather_forecast,\n",
    "    \"url_to_markdown\": url_to_markdown,\n",
    "    \"web_search\": web_search,\n",
    "    \"get_customer_order\": get_customer_order\n",
    "}"
   ],
   "id": "d1b5799ed12b15c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Components",
   "id": "37ce06eb2c35ce48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, List, Any\n",
    "\n",
    "class Component(BaseModel):\n",
    "    name: str\n",
    "    description: Optional[str]\n",
    "    input_type: Optional[str] = \"text\"\n",
    "    output_type: Optional[str] = \"text\"\n",
    "    parameters: Dict[str, Any]\n",
    "    logs: Dict[str, List] = Field(default_factory=lambda: {\"input\": [], \"output\": []})\n",
    "\n",
    "    @log_execution\n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execution script for each component.\"\"\"\n",
    "        raise NotImplementedError(\"Component execution logic should be implemented in subclasses.\")\n",
    "\n",
    "class WebSearchComponent(Component):\n",
    "    search_engine: Optional[str] = \"google\"\n",
    "\n",
    "    @log_execution\n",
    "    def execute(self, query: str) -> Any:\n",
    "        \"\"\"Performs a web search using Google or Bing API.\"\"\"\n",
    "        return web_search(query)"
   ],
   "id": "78119f5e972f5ad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing the WebSearchComponent",
   "id": "f5dfa01d08514c13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "search_component = WebSearchComponent(name= \"WebSearch\",\n",
    "                   description= \"Search the web for information.\",\n",
    "                    search_engine=\"google\",\n",
    "                   parameters={\"query\": \"string\"})"
   ],
   "id": "b3a431ed92b2fb63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# search_component.execute(query=\"What is the latest news on Toshiba?\")\n",
    "pass"
   ],
   "id": "aafb537aac6d3f12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# print(search_component.logs[\"output\"][0])",
   "id": "14d3d133aadac129"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Object",
   "id": "43f06a8a523d165f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime\n",
    "from typing import Optional, Dict, List\n",
    "import uuid\n",
    "import pydantic\n",
    "\n",
    "class PromptReadListRequest(pydantic.BaseModel):\n",
    "    app_name: str\n",
    "    prompt_label: str\n",
    "\n",
    "class PromptReadDeployedRequest(pydantic.BaseModel):\n",
    "    app_name: str\n",
    "    prompt_label: str\n",
    "\n",
    "class PromptReadRequest(pydantic.BaseModel):\n",
    "    app_name: str\n",
    "    pid: uuid.UUID\n",
    "\n",
    "class PromptObjectRequest(pydantic.BaseModel):\n",
    "    \"\"\"\n",
    "        pid: randomUUID(),\n",
    "        app_name: appName,\n",
    "        vendor: selectedVendor,\n",
    "        model_name: selectedModelName,\n",
    "        prompt_label: selectedPromptLabel,\n",
    "        prompt: selectedPrompt,\n",
    "        hash: hash,\n",
    "        temperature: temperature,\n",
    "        max_tokens: maxTokens,\n",
    "        version: version,\n",
    "        is_deployed: false,\n",
    "        deployed_time: new Date().toISOString(),\n",
    "    \"\"\"\n",
    "    pid: uuid.UUID\n",
    "    is_system_prompt: Optional[bool]\n",
    "    app_name: str\n",
    "    vendor: str\n",
    "    model_name: str\n",
    "    prompt_label: str\n",
    "    prompt: str\n",
    "    hash: str\n",
    "    temperature: str\n",
    "    max_tokens: str\n",
    "    version: Optional[str]\n",
    "    tags: Optional[List[str]]\n",
    "    is_deployed: Optional[bool]\n",
    "    hyper_parameters: Optional[Dict[str, str]]\n",
    "    variables: Optional[List[str]]\n",
    "    deployed_time: Optional[datetime.datetime]\n",
    "\n",
    "class PromptDeployRequest(pydantic.BaseModel):\n",
    "    \"\"\"\n",
    "        pid: randomUUID(),\n",
    "        app_name: appName,\n",
    "    \"\"\"\n",
    "    pid: uuid.UUID\n",
    "    app_name: str\n",
    "\n",
    "\n",
    "class PromptObject(pydantic.BaseModel):\n",
    "    pid: uuid.UUID\n",
    "    prompt_label: str\n",
    "    prompt: str\n",
    "    sha_hash: str\n",
    "    uniqueLabel:str\n",
    "    appName:str\n",
    "    version:str\n",
    "    createdTime:datetime.datetime\n",
    "    deployedTime:Optional[datetime.datetime]\n",
    "    last_deployed:Optional[datetime.datetime]\n",
    "    modelProvider: str\n",
    "    modelName: str\n",
    "    isDeployed:bool\n",
    "    tags: Optional[List[str]]\n",
    "    hyper_parameters: Optional[Dict[str, str]]\n",
    "    variables: Optional[Dict[str, str]]\n",
    "\n"
   ],
   "id": "a8df7ae2c2908d94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9787f8c884a0a85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent",
   "id": "c066fe68d44dae45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    name: str\n",
    "    agent_id: uuid.UUID\n",
    "    parent_agent: Optional[uuid.UUID] = None\n",
    "    system_prompt: PromptObject\n",
    "    persona: Optional[str]\n",
    "    functions: List[Dict[str, Any]]\n",
    "    routing_options: Dict[str, str]\n",
    "    short_term_memory: bool = False\n",
    "    long_term_memory: bool = False\n",
    "    reasoning: bool = False\n",
    "    # input_type: Optional[Literal[\"text\", \"voice\", \"image\"]] = \"text\"\n",
    "    # output_type: Optional[Literal[\"text\", \"voice\", \"image\"]] = \"text\"\n",
    "    # Make input output type a list of types\n",
    "    input_type: Optional[List[Literal[\"text\", \"voice\", \"image\"]]] = [\"text\", \"voice\"]\n",
    "    output_type: Optional[List[Literal[\"text\", \"voice\", \"image\"]]] = [\"text\", \"voice\"]\n",
    "    response_type: Optional[Literal[\"json\", \"yaml\", \"markdown\", \"HTML\", \"None\"]] = \"json\"\n",
    "    # agent_type: Optional[Literal[\"agent\", \"workflow\"]] = \"agent\"\n",
    "\n",
    "    # Execution parameters\n",
    "    max_retries: int = 3\n",
    "    timeout: Optional[int] = None\n",
    "    deployed: bool = False\n",
    "    status: Literal[\"active\", \"paused\", \"terminated\"] = \"active\"\n",
    "    priority: Optional[int] = None\n",
    "\n",
    "    # Agent, Human or Parent Agent\n",
    "    failure_strategies: Optional[List[str]]\n",
    "\n",
    "    # Logging and monitoring\n",
    "    session_id: Optional[str] = None\n",
    "    last_active: Optional[datetime] = None\n",
    "    # logging_level: Optional[Literal[\"debug\", \"info\", \"warning\", \"error\"]] = \"info\"  # Debug level\n",
    "\n",
    "    collaboration_mode: Optional[Literal[\"single\", \"team\", \"parallel\", \"sequential\"]] = \"single\"  # Multi-agent behavior\n",
    "\n",
    "    # Add a function to import the right prompt for the agent.\n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execution script for each component.\"\"\"\n",
    "        raise NotImplementedError(\"Component execution logic should be implemented in subclasses.\")\n",
    "\n",
    "\n"
   ],
   "id": "a37524832c82ba34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "agent_system_prompt = PromptObject(pid=uuid.uuid4(),\n",
    "                                             prompt_label=\"Stock Analytics\",\n",
    "                                             prompt=\"You're a helpful assistant.\",\n",
    "                                             sha_hash=\"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8c\",\n",
    "                                             uniqueLabel=\"StockAnalytics\",\n",
    "                                             appName=\"Berkshire Hathaway\",\n",
    "                                             version=\"1.0\",\n",
    "                                             createdTime=datetime.now(),\n",
    "                                             deployedTime=None,\n",
    "                                             last_deployed=None,\n",
    "                                             modelProvider=\"OpenAI\",\n",
    "                                             modelName=\"GPT-4o-mini\",\n",
    "                                             isDeployed=False,\n",
    "                                             tags=[\"search\", \"web\"],\n",
    "                                             hyper_parameters={\"temperature\": \"0.7\"},\n",
    "                                             variables={\"search_engine\": \"google\"})"
   ],
   "id": "748ea0f85fba2acb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent Schema",
   "id": "4ade29ade1e33712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Agent Schema function\n",
    "\n",
    "import inspect\n",
    "import json\n",
    "from typing import Any, Dict, List, Union, get_type_hints\n",
    "\n",
    "def agent_schema(cls):\n",
    "    \"\"\"\n",
    "    Decorator that generates an OpenAI function schema for an agent and attaches it to the class.\n",
    "    The tool name will be the agent class name, not the 'execute' function.\n",
    "    \"\"\"\n",
    "    def python_function_to_openai_schema(agent_cls) -> Dict[str, Any]:\n",
    "        \"\"\"Converts an agent class into an OpenAI JSON schema based on its `execute` method.\"\"\"\n",
    "        execute_method = getattr(agent_cls, \"execute\", None)\n",
    "        if execute_method is None:\n",
    "            raise ValueError(f\"{agent_cls.__name__} must have an 'execute' method.\")\n",
    "\n",
    "        signature = inspect.signature(execute_method)\n",
    "        type_hints = get_type_hints(execute_method)\n",
    "\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": agent_cls.__name__,  # Tool name = Class name\n",
    "                \"description\": execute_method.__doc__ or f\"Agent {agent_cls.__name__} execution function\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for param_name, param in signature.parameters.items():\n",
    "            if param_name == \"self\":\n",
    "                continue  # Skip 'self' parameter\n",
    "\n",
    "            param_type = type_hints.get(param_name, Any)\n",
    "            openai_type, is_optional = python_type_to_openai_type(param_type)\n",
    "\n",
    "            schema[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n",
    "                \"type\": openai_type,\n",
    "                \"description\": f\"{param_name} parameter\"\n",
    "            }\n",
    "\n",
    "            if not is_optional and param.default is inspect.Parameter.empty:\n",
    "                schema[\"function\"][\"parameters\"][\"required\"].append(param_name)\n",
    "\n",
    "        return schema\n",
    "\n",
    "    def python_type_to_openai_type(py_type) -> (str, bool):\n",
    "        \"\"\"Maps Python types to OpenAI JSON schema types, handling Optional and List types.\"\"\"\n",
    "        from typing import get_origin, get_args\n",
    "\n",
    "        if get_origin(py_type) is Union:\n",
    "            args = get_args(py_type)\n",
    "            non_none_types = [t for t in args if t is not type(None)]\n",
    "            if len(non_none_types) == 1:\n",
    "                openai_type, _ = python_type_to_openai_type(non_none_types[0])\n",
    "                return openai_type, True  # It's Optional\n",
    "\n",
    "        if get_origin(py_type) is list or get_origin(py_type) is List:\n",
    "            return \"array\", False\n",
    "\n",
    "        mapping = {\n",
    "            int: \"integer\",\n",
    "            float: \"number\",\n",
    "            str: \"string\",\n",
    "            bool: \"boolean\",\n",
    "            dict: \"object\",\n",
    "        }\n",
    "\n",
    "        return mapping.get(py_type, \"string\"), False  # Default to string\n",
    "\n",
    "    # Attach schema as a class attribute\n",
    "    cls.openai_schema = python_function_to_openai_schema(cls)\n",
    "    return cls\n",
    "\n"
   ],
   "id": "afc65bff4dabf5e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@agent_schema\n",
    "class WebAgent(Agent):\n",
    "    def execute(self, query: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Ask the agent anything related to arithmetic or web search and it will try to answer it.\n",
    "        \"\"\"\n",
    "        tries = 0\n",
    "        system_prompt = self.system_prompt.prompt + f\"\"\"\n",
    "        Here are the routing options:\n",
    "        {\"\\n\".join([f\"{k}: {v}\" for k, v in self.routing_options.items()])}\n",
    "\n",
    "        Your response should be in the format:\n",
    "        {{ \"routing\": \"respond\", \"content\": \"The answer to the query.\"}}\n",
    "\n",
    "        \"\"\"\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        while tries < self.max_retries:\n",
    "            print(tries)\n",
    "            print(messages)\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages,\n",
    "                    functions=self.functions,\n",
    "                    # tools=self.functions,\n",
    "                    # parallel_tool_calls=True,\n",
    "                )\n",
    "                print(response)\n",
    "                if response.choices[0].message.function_call:\n",
    "                    print(response.choices[0].message.function_call)\n",
    "                    # print(**response.choices[0].message.function_call.arguments)\n",
    "                    arguments = json.loads(response.choices[0].message.function_call.arguments)\n",
    "                    function_name = response.choices[0].message.function_call.name\n",
    "                    output = tool_store[function_name](**arguments)\n",
    "                    # output = \"Toshiba is a great company\"\n",
    "                    # print(output)\n",
    "                    messages+=[{\"role\":\"system\", \"content\": f\"I called the function {function_name} with the arguments {arguments} and got the output {output}.\"},\n",
    "                        {\"role\": \"user\", \"content\": \"\"\"Now, try to answer the original query given the output. If you can't, try using another tool. If it does answer the query, you can stop here and just answer the query. If you can't then you can give up.\n",
    "                    \"\"\"}]\n",
    "                else:\n",
    "                    return response.choices[0].message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            tries += 1"
   ],
   "id": "4ad3aef96337c0a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "WebAgent.openai_schema",
   "id": "5ca3136d21b36a41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent with sequential tool calling",
   "id": "79946d3ba8ab61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class WebAgent(Agent):\n",
    "    @function_schema\n",
    "    def execute(self, query: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Ask the agent anything related to arithmetic or web search and it will try to answer it.\n",
    "        \"\"\"\n",
    "        tries = 0\n",
    "        system_prompt = self.system_prompt.prompt + f\"\"\"\n",
    "        Here are the routing options:\n",
    "        {\"\\n\".join([f\"{k}: {v}\" for k, v in self.routing_options.items()])}\n",
    "\n",
    "        Your response should be in the format:\n",
    "        {{ \"routing\": \"respond\", \"content\": \"The answer to the query.\"}}\n",
    "\n",
    "        \"\"\"\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        while tries < self.max_retries:\n",
    "            print(tries)\n",
    "            print(messages)\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages,\n",
    "                    functions=self.functions,\n",
    "                    # tools=self.functions,\n",
    "                    # parallel_tool_calls=True,\n",
    "                )\n",
    "                print(response)\n",
    "                if response.choices[0].message.function_call:\n",
    "                    print(response.choices[0].message.function_call)\n",
    "                    # print(**response.choices[0].message.function_call.arguments)\n",
    "                    arguments = json.loads(response.choices[0].message.function_call.arguments)\n",
    "                    function_name = response.choices[0].message.function_call.name\n",
    "                    output = tool_store[function_name](**arguments)\n",
    "                    # output = \"Toshiba is a great company\"\n",
    "                    # print(output)\n",
    "                    messages+=[{\"role\":\"system\", \"content\": f\"I called the function {function_name} with the arguments {arguments} and got the output {output}.\"},\n",
    "                        {\"role\": \"user\", \"content\": \"\"\"Now, try to answer the original query given the output. If you can't, try using another tool. If it does answer the query, you can stop here and just answer the query. If you can't then you can give up.\n",
    "                    \"\"\"}]\n",
    "                else:\n",
    "                    return response.choices[0].message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            tries += 1"
   ],
   "id": "97da404c515556c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent with parallel tool calling",
   "id": "5707425d80125d90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@agent_schema\n",
    "class WebAgent(Agent):\n",
    "    def execute(self, query: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Ask the agent anything related to arithmetic, customer orders numbers or web search and it will try to answer it.\n",
    "        You can ask it multiple questions at once. No need to ask one question at a time. You can ask it for multiple customer ids, multiple arithmetic questions, or multiple web search queries.\n",
    "\n",
    "        You can ask:\n",
    "        query : what are the customer order numbers for customer id 1111 and 2222.\n",
    "        query : what is the sum of 2 and 3, and sum of 12 and 13.\n",
    "        query : what is the latest news on Toshiba and Apple.\n",
    "        query : what is the sum of 12 and 13, and web results for \"latest news on Toshiba.\n",
    "        \"\"\"\n",
    "        tries = 0\n",
    "        system_prompt = self.system_prompt.prompt + f\"\"\"\n",
    "        Here are the routing options:\n",
    "        {\"\\n\".join([f\"{k}: {v}\" for k, v in self.routing_options.items()])}\n",
    "\n",
    "        Your response should be in the format:\n",
    "        {{ \"routing\": \"respond\", \"content\": \"The answer to the query.\"}}\n",
    "\n",
    "        \"\"\"\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        while tries < self.max_retries:\n",
    "            print(tries)\n",
    "            # print(\"\\n\\nMessage: \",messages)\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages,\n",
    "                    # functions=self.functions,\n",
    "                    tools=self.functions,\n",
    "                    # parallel_tool_calls=True,\n",
    "                    tool_choice=\"auto\",\n",
    "                )\n",
    "                print(\"\\n\\nResponse: \",response)\n",
    "                if response.choices[0].finish_reason==\"tool_calls\":\n",
    "                    tool_calls = response.choices[0].message.tool_calls\n",
    "                    messages+=[{\"role\": \"assistant\", \"tool_calls\": tool_calls},]\n",
    "                    for tool in tool_calls:\n",
    "                        print(tool.function.name)\n",
    "                        tool_id = tool.id\n",
    "                        arguments = json.loads(tool.function.arguments)\n",
    "                        function_name = tool.function.name\n",
    "                        result = tool_store[function_name](**arguments)\n",
    "                        print(result)\n",
    "                        messages+= [{\"role\": \"tool\", \"tool_call_id\": tool_id, \"content\": str(result)}]\n",
    "\n",
    "                else:\n",
    "                    return response.choices[0].message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            tries += 1"
   ],
   "id": "a0f14b4233763e2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "WebAgent.openai_schema",
   "id": "7d16635dac62cd01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "web_agent = WebAgent(name=\"WebSearchAgent\",\n",
    "                agent_id=uuid.uuid4(),\n",
    "                system_prompt=agent_system_prompt,\n",
    "                persona=\"Helper\",\n",
    "                functions=[web_search.openai_schema, add_numbers.openai_schema, get_customer_order.openai_schema],\n",
    "                routing_options={\"continue\": \"If you think you can't answer the query, you can continue to the next tool or do some reasoning.\",\n",
    "                                 \"respond\": \"If you think you have the answer, you can stop here.\",\n",
    "                                 \"give_up\": \"If you think you can't answer the query, you can give up and let the user know.\"\n",
    "                                 },\n",
    "\n",
    "                short_term_memory=True,\n",
    "                long_term_memory=False,\n",
    "                reasoning=False,\n",
    "                input_type=[\"text\", \"voice\"],\n",
    "                output_type=[\"text\", \"voice\"],\n",
    "                response_type=\"json\",\n",
    "                max_retries=5,\n",
    "                timeout=None,\n",
    "                deployed=False,\n",
    "                status=\"active\",\n",
    "                priority=None,\n",
    "                failure_strategies=[\"retry\", \"escalate\"],\n",
    "                session_id=None,\n",
    "                last_active=datetime.now(),\n",
    "                collaboration_mode=\"single\"\n",
    "                )"
   ],
   "id": "6c722caa3c7f8e4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# web_agent.execute.openai_schema['function']['name'] = \"web_agent\"\n",
    "# del web_agent.execute.openai_schema['function']['parameters']['properties'][\"self\"]\n",
    "# web_agent.execute.openai_schema['function']['parameters']['required'].pop(0)\n",
    "# web_agent.execute.openai_schema"
   ],
   "id": "34a3c056eff96ba0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "agent_store = {\n",
    "    \"WebAgent\": web_agent.execute\n",
    "}"
   ],
   "id": "272116e8e034cb70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add an execution method for the agent\n",
    "res = web_agent.execute(\"Find the order number of customer ID 1111 and get the order number of customer of 2222. Find the sum of the order numbers, and search about the sum on the web. Tell me what you found on the web.\")\n",
    "# res = stock_agent.execute(\"What is the projected revenue for QBTS in 2025. Add its revenue with 195900?\")\n",
    "# res = stock_agent.execute(\"What is the latest news on Toshiba in March 2025?\")\n",
    "# res = stock_agent.execute(\"Prove Goldbach Conjecture\")\n",
    "print(res)\n"
   ],
   "id": "fd615c39d99a957b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "res",
   "id": "d42e5b614a279bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(json.loads(res)[\"content\"])",
   "id": "9e4544d43d81116b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Command Agent",
   "id": "d0e8d83c9aec9cc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "web_agent.openai_schema",
   "id": "5a7ab17338b1dfb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "command_agent_system_prompt = PromptObject(pid=uuid.uuid4(),\n",
    "                                                prompt_label=\"Command Agent\",\n",
    "                                                prompt=\"\"\"You're a command agent. Assign proper tasks to the agents and look at their outputs to generate a final output. Try to ask multiple questions at once to the agent that is capable of doing so. For instance, if the agent is capable of doing three things, ask the agent to do the three things at once.\n",
    "\n",
    "                                                Remeember, if an agent responds with the routing option \"respond\" then it means they are done with the task. If they respond with \"continue\" then they are not done with the task and you can ask them to do more tasks. If they respond with \"give_up\" then they are not able to do the task and you can ask another agent to do the task or tell the user you can't do it if the agents give up a lot of the times.\n",
    "                                                \"\"\",\n",
    "                                                sha_hash=\"000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8c\",\n",
    "                                                uniqueLabel=\"CommandAgent\",\n",
    "                                                appName=\"iopex\",\n",
    "                                                version=\"1.0\",\n",
    "                                                createdTime=datetime.now(),\n",
    "                                                deployedTime=None,\n",
    "                                                last_deployed=None,\n",
    "                                                modelProvider=\"OpenAI\",\n",
    "                                                modelName=\"GPT-4o-mini\",\n",
    "                                                isDeployed=False,\n",
    "                                                tags=[\"search\", \"web\"],\n",
    "                                                hyper_parameters={\"temperature\": \"0.7\"},\n",
    "                                                variables={\"search_engine\": \"google\"})"
   ],
   "id": "d98a832686d08fd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CommandAgent(Agent):\n",
    "    def execute(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        This agent can call any agent, tool, or component. It can also call a router agent to call multiple agents.\n",
    "        \"\"\"\n",
    "        tries = 0\n",
    "        system_prompt = self.system_prompt.prompt\n",
    "\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        while tries < self.max_retries:\n",
    "            print(\"\\n\\nCommand Agent Tries: \",tries)\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"o3-mini\",\n",
    "                    messages=messages,\n",
    "                    # temperature=float(self.system_prompt.hyper_parameters[\"temperature\"]),\n",
    "                    # functions=self.functions,\n",
    "                    tools=self.functions,\n",
    "                    # parallel_tool_calls=True,\n",
    "                    tool_choice=\"auto\",\n",
    "                )\n",
    "                print(\"\\n\\nResponse: \",response)\n",
    "                if response.choices[0].finish_reason==\"tool_calls\":\n",
    "                    tool_calls = response.choices[0].message.tool_calls\n",
    "                    messages+=[{\"role\": \"assistant\", \"tool_calls\": tool_calls},]\n",
    "                    for tool in tool_calls:\n",
    "                        print(\"\\n\\nAgent Called: \",tool)\n",
    "                        tool_id = tool.id\n",
    "                        arguments = json.loads(tool.function.arguments)\n",
    "                        function_name = tool.function.name\n",
    "                        result = agent_store[function_name](**arguments)\n",
    "                        print(result)\n",
    "                        messages+= [{\"role\": \"tool\", \"tool_call_id\": tool_id, \"content\": str(result)}]\n",
    "\n",
    "                else:\n",
    "                    return response.choices[0].message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            tries += 1\n"
   ],
   "id": "31f7e6950dc2faf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "web_command_agent = CommandAgent(name=\"WebCommandAgent\",\n",
    "                agent_id=uuid.uuid4(),\n",
    "                system_prompt=command_agent_system_prompt,\n",
    "                persona=\"Command Agent\",\n",
    "                functions=[web_agent.openai_schema],\n",
    "                routing_options={\"continue\": \"If you think you can't answer the query, you can continue to the next tool or do some reasoning.\",\n",
    "                                 \"respond\": \"If you think you have the answer, you can stop here.\",\n",
    "                                 \"give_up\": \"If you think you can't answer the query, you can give up and let the user know.\"\n",
    "                                 },\n",
    "\n",
    "                short_term_memory=True,\n",
    "                long_term_memory=False,\n",
    "                reasoning=False,\n",
    "                input_type=[\"text\", \"voice\"],\n",
    "                output_type=[\"text\", \"voice\"],\n",
    "                response_type=\"json\",\n",
    "                max_retries=5,\n",
    "                timeout=None,\n",
    "                deployed=False,\n",
    "                status=\"active\",\n",
    "                priority=None,\n",
    "                failure_strategies=[\"retry\", \"escalate\"],\n",
    "                session_id=None,\n",
    "                last_active=datetime.now(),\n",
    "                collaboration_mode=\"single\"\n",
    "                )"
   ],
   "id": "99fe3bcd14971eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "result_agent =web_command_agent.execute(\"Find the order number of customer ID 1111 and get the order number of customer of 2222. Find the sum of the order numbers, and search about the sum on the web. Tell me what you found on the web.\")",
   "id": "fbc2b13546cee261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(result_agent)",
   "id": "ec28ba10a993731b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### APIs",
   "id": "dd9c770f112c85e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T18:01:30.994255Z",
     "start_time": "2025-03-27T18:01:28.458991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env.local\")\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # response_format={\"type\": \"json_schema\", \"json_schema\": '{\"animal_type\": \"animal name\"}'},\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "ceaf25d9b9d993e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a black puppy, likely a Labrador. It appears to be lying on a wooden surface, with a focused expression. The puppy has a shiny coat and big, expressive eyes.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f92e2c73831ab195"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}

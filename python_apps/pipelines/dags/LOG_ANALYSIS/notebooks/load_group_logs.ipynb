{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_file = \"/home/k/airflow/dags/dags/LOG_ANALYSIS/notebooks/swlog_04\"\n",
    "output_file = \"group.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from typing import List, Tuple\n",
    "\n",
    "class LogParser:\n",
    "    def __init__(self, k_prefix=3, similarity_threshold=0.5, client=None, max_words=70):\n",
    "        self.k_prefix = k_prefix\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.groups = {}\n",
    "        self.template_memory = []\n",
    "        self.client = client\n",
    "        self.ground_truth = {}\n",
    "        self.num_logs = 0\n",
    "        self.max_words = max_words\n",
    "        \n",
    "    def get_group(self, token_length, prefix):\n",
    "        if token_length not in self.groups:\n",
    "            self.groups[token_length] = {}\n",
    "        if prefix not in self.groups[token_length]:\n",
    "            self.groups[token_length][prefix] = []\n",
    "        return self.groups[token_length][prefix]\n",
    "        \n",
    "    def load_log_file(self, file_path: str, num_prefix: int = 5) -> List[Tuple[str, str, str]]:\n",
    "        processed_logs = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            logs = file.readlines()\n",
    "            for log in logs:\n",
    "                self.num_logs += 1\n",
    "                parts = log.split(' ', num_prefix)\n",
    "                if len(parts) > num_prefix:\n",
    "                    datetime = ' '.join(parts[:num_prefix-1])\n",
    "                    switch_id = parts[num_prefix-1]\n",
    "                    log_entry = parts[num_prefix]\n",
    "                    processed_logs.append((datetime, switch_id, log_entry.strip()))\n",
    "        return processed_logs\n",
    "\n",
    "    def _limit_words(self, log_entries):\n",
    "        limited_logs = []\n",
    "        total_words = 0\n",
    "        for log in log_entries:\n",
    "            word_count = len(log.split())\n",
    "            if total_words + word_count > self.max_words:\n",
    "                break\n",
    "            limited_logs.append(log)\n",
    "            total_words += word_count\n",
    "        return limited_logs, total_words\n",
    "        \n",
    "    def tokenize(self, log):\n",
    "        return [re.sub(r'\\d+', '*', token) for token in log.split()]\n",
    "\n",
    "    def calculate_similarity(self, tokens1, tokens2):\n",
    "        set1, set2 = set(tokens1), set(tokens2)\n",
    "        if len(set1) == 0 or len(set2) == 0:\n",
    "            return 0 \n",
    "        common_tokens = set1.intersection(set2)\n",
    "        return len(common_tokens) / len(set1.union(set2))\n",
    "\n",
    "    def process_log(self, log, datetime, switch_id):\n",
    "        tokens = self.tokenize(log)\n",
    "        token_length = len(tokens)\n",
    "        matched_template = self.search_template_memory(log, token_length)\n",
    "        if matched_template:\n",
    "            return\n",
    "    \n",
    "        prefix = tuple(tokens[:self.k_prefix])\n",
    "        group = self.get_group(token_length, prefix)\n",
    "    \n",
    "        found_group = False\n",
    "        for g in group:\n",
    "            if self.calculate_similarity(g['example'], tokens) > self.similarity_threshold:\n",
    "                g['logs'].append((datetime, switch_id, log))\n",
    "                found_group = True\n",
    "                break\n",
    "    \n",
    "        if not found_group:\n",
    "            group.append({'example': tokens, 'logs': [(datetime, switch_id, log)]})\n",
    "\n",
    "    def search_template_memory(self, log, token_length):\n",
    "        possible_templates = [t for t in self.template_memory if len(t.split()) <= token_length]\n",
    "        for template in possible_templates:\n",
    "            if re.match(template, log):\n",
    "                return template\n",
    "        return None\n",
    "\n",
    "    def add_to_template_memory(self, template):\n",
    "        regex_template = re.sub(r'<\\*>', '(.*?)', template)\n",
    "        self.template_memory.append(regex_template)\n",
    "\n",
    "    def _call_llm_api(self, logs):\n",
    "        log_entries = [log[2] for log in logs]\n",
    "        limited_log_entries, total_words = self._limit_words(log_entries)\n",
    "        print(\"Input_tokens here : \", total_words)\n",
    "        input_data = ' '.join(limited_log_entries)\n",
    "        \n",
    "        json_data = {\n",
    "                    \"args\": [f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "                                You are a helpful assistant to extract template string from the log entries.\n",
    "                                <|eot_id|>\n",
    "                                <|start_header_id|>user<|end_header_id|>\n",
    "                                Please answer the following\n",
    "                                <|prompt|>\n",
    "                                        ### Instruction ###\n",
    "                                        You will be provided with a list of logs. You must identify and abstract\n",
    "                                        all the dynamic variables in logs with \"<*>\" and output ONE static log\n",
    "                                        template that matches all the logs. Print ONLY ONE input logsâ€™ template\n",
    "                                        delimited by backticks. NO description in output is needed. The template\n",
    "                                        needs to be exactly correct to be matched to the templates so that we can\n",
    "                                        use it for template matching without unnecessary puntuations or letters.\n",
    "                                        ### Standardizing LLM Response by Input and Output Example ###\n",
    "                                        \n",
    "                                        Example Input 1\n",
    "                                        Log List: [\"try to connected to host: 172.16.254.1, finished.\", \"try to connected to host: 173.16.254.2, finished.\"]\n",
    "                                        Example Output 1\n",
    "                                        `try to connected to host: <*>, finished.`\n",
    "                                        \n",
    "                                        Example Input 2\n",
    "                                        Log List: [\"OMN-LabStock-15 swlogd SES AAA INFO: Login by centreon_ssh_admin_v2 from 10.172.6.1 through SSH Success [in LoginAaaSession::handleLoginResult()]\", \"OMN-LabStock-15 swlogd SES AAA INFO: Login by centreon_ssh_admin_v2 from 10.172.6.1 through SSH Success [in LoginAaaSession::handleLoginResult()]\"]\n",
    "                                        Example Output 2\n",
    "                                        `OMN-LabStock-15 swlogd SES AAA INFO: Login by centreon_ssh_admin_v2 from <*> through SSH Success [in LoginAaaSession::handleLoginResult()]`\n",
    "                                        \n",
    "                                        Example Input 3\n",
    "                                        Log List: [\"OMN-LabStock-15 sshd[551] Received keyboard-interactive/pam for centreon_ssh_admin_v2 from 10.172.6.1 port 21577 ssh2\", \"OMN-LabStock-15 sshd[7731] Received keyboard-interactive/pam for centreon_ssh_admin_v2 from 10.172.6.1 port 50385 ssh2\"]\n",
    "                                        Example Output 3\n",
    "                                        `OMN-LabStock-15 sshd[<*>] Received keyboard-interactive/pam for centreon_ssh_admin_v2 from <*> port <*> ssh2`\n",
    "                                      \n",
    "                                        Example Input 4\n",
    "                                        Log List: [\"OMN-LabStock-15 sshd[589] Disconnected from user centreon_ssh_admin_v2 10.172.6.1 port 21577\", \"OMN-LabStock-15 sshd[7768] Disconnected from user centreon_ssh_admin_v2 10.172.6.1 port 50385\"]\n",
    "                                        Example Output 4\n",
    "                                        `OMN-LabStock-15 sshd[<*>] Disconnected from user centreon_ssh_admin_v2 <*> port <*>`\n",
    "                                        \n",
    "                                        Example Input 5\n",
    "                                        Log List: [\"OMN-LabStock-15 swlogd svcCmm mBIND INFO: smgrIsisTransitSvc@126 Service for isid 2121 is found\", \"OMN-LabStock-15 swlogd svcCmm mBIND INFO: smgrIsisTransitSvc@126 Service for isid 2117 is found\"]\n",
    "                                        Example Output 5\n",
    "                                        `OMN-LabStock-15 swlogd svcCmm mBIND INFO: smgrIsisTransitSvc@126 Service for isid <*> is found`\n",
    "                                        \n",
    "                                        Example Input 6\n",
    "                                        Log List: [\"sshd[551] Received keyboard-interactive/pam for centreon_ssh_admin_v2 from 10.172.6.1 port 21577 ssh2\"]\n",
    "                                        Example Output 6\n",
    "                                        `sshd[<*>] Received keyboard-interactive/pam for centreon_ssh_admin_v2 from <*> port <*> ssh2`\n",
    "                                        \n",
    "                                        ### Retrieval-Augmented Log Parsing ###\n",
    "                                        Log List: \n",
    "                                {input_data}\n",
    "                                <|prompt|>\n",
    "                                <|eot_id|>\n",
    "                                <|start_header_id|>assistant<|end_header_id|>\n",
    "                            \"\"\"],\n",
    "                    \"kwargs\": {\n",
    "                    \"max_new_tokens\": 8000,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"temperature\": 0.01,\n",
    "                    \"do_sample\": False,\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            username = \"admin\"\n",
    "            password = \"HtEgv1PsExDxaRMllBioFj7PnJUDl4dU\"\n",
    "            auth = HTTPBasicAuth(username, password)\n",
    "            response = requests.post('https://elevaite-prodmodelapi.iopex.ai/inference/3/infer', json=json_data, auth=auth, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                template = data.get('result', '')[0].get('generated_text', '')\n",
    "                print(\"Template : \", template)\n",
    "                return template\n",
    "            else:\n",
    "                print(f\"API call failed with status code {response.status_code}\")\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to call local LLM API: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def group_logs(self, log_tuples):\n",
    "        for datetime, switch_id, log in log_tuples:\n",
    "            self.process_log(log, datetime, switch_id)\n",
    "\n",
    "    def parse_logs_with_llm(self):\n",
    "        for length, prefix_groups in self.groups.items():\n",
    "            for prefix, groups in prefix_groups.items():\n",
    "                for group in groups:\n",
    "                    log_list = group['logs']\n",
    "                    template = self._call_llm_api(log_list)\n",
    "                    if template:\n",
    "                        group['template'] = template\n",
    "\n",
    "    def _generate_topic_with_llm(self, logs):\n",
    "        log_entries = [log[2] for log in logs]\n",
    "        limited_log_entries, total_words = self._limit_words(log_entries)\n",
    "        print(\"Input Log Group tokens :\", total_words)\n",
    "        input_data = ' '.join(limited_log_entries)\n",
    "\n",
    "        json_data = {\n",
    "                \"args\": [f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "                        You are a helpful assistant to generate topics for log entries.\n",
    "                            ### Instruction ###\n",
    "                            You will be provided with a list of logs. Please generate a short topic\n",
    "                            that summarizes what these logs are about in simple words.\n",
    "                            Output ONLY the topic within backticks, NO description needed.\n",
    "                            Give me example logs as a follow-up.\n",
    "                            ### Example ###\n",
    "                            Log List: [\"Error connecting to database.\", \"Database connection timed out.\"]\n",
    "                            Topic: `Database Connection Issues`, Example Logs: [\"Error Connecting to database.\"]\n",
    "                            ### Logs ###\n",
    "                        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                        \n",
    "                        <|context|>\n",
    "                        \n",
    "                        {input_data}\n",
    "                        \n",
    "                        <|context|>\n",
    "                        <|start_header_id|>assistant<|end_header_id|>\n",
    "                    \"\"\"],\n",
    "                \"kwargs\": {\n",
    "                \"max_new_tokens\": 8000,\n",
    "                \"return_full_text\": False,\n",
    "                \"temperature\": 0.01,\n",
    "                \"do_sample\": False,\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            username = \"admin\"\n",
    "            password = \"HtEgv1PsExDxaRMllBioFj7PnJUDl4dU\"\n",
    "            auth = HTTPBasicAuth(username, password)\n",
    "            response = requests.post('https://elevaite-prodmodelapi.iopex.ai/inference/3/infer', json=json_data, auth=auth, verify=False)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                topic = data.get('result', '')[0].get('generated_text', '')\n",
    "                print(\"Topic : \", topic)\n",
    "                return topic\n",
    "            else:\n",
    "                print(f\"API call failed with status code {response.status_code}\")\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to call local LLM API: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_topics(self):\n",
    "        for length, prefix_groups in self.groups.items():\n",
    "            for prefix, groups in prefix_groups.items():\n",
    "                for group in groups:\n",
    "                    log_list = group['logs']\n",
    "                    topic = self._generate_topic_with_llm(log_list)\n",
    "                    if topic:\n",
    "                        group['topic'] = topic\n",
    "\n",
    "    def extract_variables(self):\n",
    "        extracted_data = []\n",
    "        for length, prefix_groups in self.groups.items():\n",
    "            for prefix, groups in prefix_groups.items():\n",
    "                for group in groups:\n",
    "                    template = group.get('template', '').replace('`', '').strip()\n",
    "                    template = template.replace('[', '\\[').replace(']', '\\]').replace('(', '\\(').replace(')', '\\)')\n",
    "                    regex_template = template.replace('<*>', '(.*?)')\n",
    "                    for datetime, switch_id, log in group['logs']:\n",
    "                        if '.' not in datetime:\n",
    "                            datetime += '.000'\n",
    "                        extracted_variables = re.match(regex_template, log)\n",
    "                        if extracted_variables:\n",
    "                            extracted_data.append({\n",
    "                                'log_datetime': datetime,\n",
    "                                'switch_id': switch_id,\n",
    "                                'raw_log_message': log,\n",
    "                                'log_template': template,\n",
    "                                'log_label': group.get('topic', ''),\n",
    "                                'log_variables': extracted_variables.groups()\n",
    "                            })\n",
    "                        else:\n",
    "                            extracted_data.append({\n",
    "                                'log_datetime': datetime,\n",
    "                                'switch_id': switch_id,\n",
    "                                'raw_log_message': log,\n",
    "                                'log_template': template,\n",
    "                                'log_label': group.get('topic', ''),\n",
    "                                'log_variables': 'None'\n",
    "                            })\n",
    "        return extracted_data\n",
    "        \n",
    "def convert_keys_to_strings(d):\n",
    "    new_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(key, tuple):\n",
    "            new_key = '_'.join(key)  # Convert tuple to a string\n",
    "        else:\n",
    "            new_key = key\n",
    "        if isinstance(value, dict):\n",
    "            new_dict[new_key] = convert_keys_to_strings(value)\n",
    "        else:\n",
    "            new_dict[new_key] = value\n",
    "    return new_dict\n",
    "    \n",
    "def load_and_process_logs(file_path: str, output_path: str):\n",
    "    parser = LogParser(k_prefix=5)\n",
    "    processed_logs = parser.load_log_file(file_path, num_prefix=5)\n",
    "    parser.group_logs(processed_logs)\n",
    "    \n",
    "        # Convert tuple keys to strings\n",
    "    groups_with_string_keys = convert_keys_to_strings(parser.groups)\n",
    "\n",
    "    # Write groups to a JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(groups_with_string_keys, f)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_file = load_and_process_logs(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable.set('load_group_logs_file', groups_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

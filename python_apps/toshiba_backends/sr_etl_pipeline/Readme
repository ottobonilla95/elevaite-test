# Enhanced ETL Pipeline for Toshiba Service Data

A production-ready ETL pipeline that processes service request data from S3 and loads it into PostgreSQL with 99.4% reliability.

## Overview

This pipeline transforms Toshiba service data from CSV files stored in S3 into structured PostgreSQL tables for analytics and dashboard consumption. The enhanced version resolves critical issues with CSV parsing, data loss, and provides enterprise-grade logging and monitoring.

## Key Features

- **99.4% Success Rate**: Processes 506/509 files successfully using robust CSV parsing
- **Incremental Loading**: Preserves existing data while adding new records
- **Robust CSV Parser**: Handles variable field counts (30-89 columns vs expected 29)
- **Production Logging**: Structured logs with rotation and monitoring integration
- **High Performance**: 2,544 rows/second throughput (4.18M rows in 27 minutes)
- **Data Integrity**: Prevents duplicates using conflict resolution

## Architecture

```
S3 Bucket (CSV Files) → Enhanced ETL Pipeline → PostgreSQL Database
                                ↓
                      Production Logs & Monitoring
```

### Data Flow
1. **S3 Connector**: Downloads CSV files from `tgcs-raw-data` bucket
2. **Enhanced Parser**: Robust CSV parsing with 4 fallback strategies
3. **Data Transformer**: Normalizes data into 5 PostgreSQL tables
4. **Incremental Loader**: Uses `ON CONFLICT DO NOTHING` for safe loading
5. **State Management**: Tracks processed files to avoid reprocessing

## Database Schema

### Tables Created
- `service_requests`: Main service ticket data (366K+ records)
- `customers`: Customer account information (1.2K+ records)
- `tasks`: Service tasks and assignments (467K+ records)
- `parts_used`: Parts and materials used (651K+ records)
- `sr_notes`: Service request notes and comments (1M+ records)

### Key Relationships
- Service Requests → Customer Account Numbers
- Tasks → Service Request Numbers
- Parts Used → Task Numbers
- Notes → Service Request Numbers

## Performance Metrics

| Metric | Value |
|--------|-------|
| Files Processed | 506/509 (99.4%) |
| Total Records | 4.18M rows |
| Processing Time | 27 minutes |
| Throughput | 2,544 rows/second |
| Data Coverage | 99.4% of available data |

## Installation & Setup

### Prerequisites
- Python 3.8+
- PostgreSQL database
- AWS S3 access credentials
- Required Python packages (see requirements.txt)

### Configuration
Create `config/settings.json`:
```json
{
    "s3_bucket": "tgcs-raw-data",
    "s3_folder": "Toshiba/Historical Tickets/",
    "aws_access_key_id": "your-key",
    "aws_secret_access_key": "your-secret",
    "aws_region": "us-east-2",
    "pg_conn_string": "postgresql+asyncpg://user:pass@host:port/db"
}
```

## Usage

### Daily Production Run
```bash
python main.py
```

### Development Mode (Detailed Logging)
```bash
# Edit utils/logger.py: change get_production_logger("DEBUG")
python main.py
```

### Processing Specific Files
The pipeline automatically:
- Identifies new/modified files
- Retries previously failed files
- Skips already processed files

## Logging & Monitoring

### Log Files
- `logs/etl_pipeline.log`: Detailed processing logs (50MB rotation)
- `logs/etl_errors.log`: Error-only logs for quick triage
- `logs/pipeline_summary_YYYYMMDD.json`: Daily summaries for monitoring

### Production Console Output
```
10:30:15 - INFO - ETL Pipeline Started - Processing 150 files
10:32:15 - INFO - Progress: 50 processed, 2 failed, 125,000 rows, 1,250 rows/sec
10:35:45 - INFO - ETL Pipeline Completed
10:35:45 - INFO - Duration: 300 seconds, Throughput: 2,400 rows/second
```

## Error Handling

### Error Classification
- `CSV_FORMAT_ERROR`: Field count mismatches (97% of previous failures)
- `SCHEMA_LENGTH_ERROR`: Fields exceeding database limits
- `MISSING_REQUIRED_COLUMNS`: Critical columns not found
- `DATABASE_CONNECTION_ERROR`: Connection issues
- `OTHER_ERROR`: Miscellaneous errors

### Robust CSV Parsing Strategies
1. **Standard Pandas**: Normal CSV parsing
2. **Python Engine**: Skip bad lines, handle malformed data
3. **Line-by-line**: Dynamic field count adaptation
4. **Max Columns**: Accommodate variable field counts

## Data Quality Features

### Field Truncation
Automatically truncates fields exceeding database limits:
- VARCHAR fields: 99 characters max
- TEXT fields: 200-1000 characters based on content type

### Duplicate Prevention
- Uses `ON CONFLICT DO NOTHING` for natural primary keys
- Tracks file processing state to prevent reprocessing
- Maintains data integrity across multiple runs

## Scaling for Production

### Current Capacity
- **Daily Processing**: Handles 500+ files efficiently
- **Data Volume**: 4M+ rows per run
- **Throughput**: 2.5K rows/second sustainable

### Enterprise Scale Recommendations
For millions of daily records:
- Implement parallel file processing
- Use bulk COPY operations
- Add connection pooling
- Consider distributed processing (Airflow/Spark)

## Troubleshooting

### Common Issues

**Pipeline shows 0% progress**
- Check S3 credentials and bucket access
- Verify database connection string
- Ensure sufficient disk space for temp files

**High failure rates**
- Check database schema matches expected data types
- Verify network connectivity to S3 and database
- Review error logs for specific failure patterns

**Performance issues**
- Monitor database connection limits
- Check available disk space for temp files
- Review batch sizes in postgres_loader.py

### Failed Files
The pipeline maintains `failed_files_log.json` with:
- Failure reasons and error types
- Retry counts and timestamps
- Detailed error messages for debugging

## File Structure

```
sr_etl_pipeline/
├── main.py                     # Enhanced pipeline orchestrator
├── config/
│   └── settings.json          # Configuration
├── connectors/
│   └── S3Connector.py         # S3 file operations
├── etl_transformers/
│   └── normalize_excel.py     # Robust CSV parser
├── loaders/
│   └── postgres_loader.py     # Incremental database loader
├── utils/
│   └── logger.py              # Production logging
├── logs/                      # Generated log files
├── temp/                      # Temporary file storage
└── README.md                  # This file
```

## Contributing

### Development Workflow
1. Create feature branch from `main`
2. Make changes and test locally
3. Update this README if needed
4. Submit pull request with detailed description

### Code Standards
- Follow existing logging patterns
- Add error handling for new features
- Update configuration examples
- Test with sample data before deployment

## Version History

### v2.0 (Enhanced Pipeline)
- Robust CSV parsing (99.4% success rate)
- Incremental loading with conflict resolution
- Production logging and monitoring
- Enterprise-ready error handling

### v1.0 (Original Pipeline)
- Basic CSV processing
- Full table recreation on each run
- ~75% success rate
- Limited error handling

## Support

For issues or questions:
1. Check the error logs in `logs/etl_errors.log`
2. Review failed files in `failed_files_log.json`
3. Consult this README for common solutions
4. Contact the data engineering team

## License

Internal use only - Toshiba Service Data Processing